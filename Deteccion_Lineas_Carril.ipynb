{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Líneas de Carril\n",
    "## Proyecto Individual - Fundamentos de la Visión por Computador\n",
    "## Dataset: Udacity Self-Driving Car Nanodegree\n",
    "\n",
    "**Universidad de Deusto - Facultad de Ingeniería**\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Utilizado\n",
    "\n",
    "Este proyecto utiliza los videos del dataset público **Udacity Self-Driving Car Nanodegree - Lane Lines Detection**:\n",
    "- Repositorio: https://github.com/udacity/CarND-LaneLines-P1\n",
    "- Videos: `solidWhiteRight.mp4`, `solidYellowLeft.mp4`\n",
    "- Resolución: 960x540 píxeles\n",
    "- Framerate: 25 FPS\n",
    "\n",
    "### Técnicas Utilizadas\n",
    "\n",
    "- Conversión a escala de grises\n",
    "- Filtrado Gaussiano (reducción de ruido)\n",
    "- Detección de bordes (algoritmo de Canny)\n",
    "- Región de interés (ROI)\n",
    "- Transformada de Hough para detección de líneas\n",
    "- Suavizado temporal\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuración de visualización\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verificar Dataset de Udacity\n",
    "\n",
    "Antes de continuar, asegúrate de haber descargado los videos del dataset de Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos del dataset Udacity Self-Driving Car\n",
    "UDACITY_VIDEOS = {\n",
    "    'solidWhiteRight.mp4': 'https://github.com/udacity/CarND-LaneLines-P1/raw/master/test_videos/solidWhiteRight.mp4',\n",
    "    'solidYellowLeft.mp4': 'https://github.com/udacity/CarND-LaneLines-P1/raw/master/test_videos/solidYellowLeft.mp4'\n",
    "}\n",
    "\n",
    "# Verificar qué videos están disponibles\n",
    "available_videos = []\n",
    "for video_name in UDACITY_VIDEOS.keys():\n",
    "    if os.path.exists(video_name):\n",
    "        available_videos.append(video_name)\n",
    "        print(f\"✓ Encontrado: {video_name}\")\n",
    "    else:\n",
    "        print(f\"✗ No encontrado: {video_name}\")\n",
    "\n",
    "if not available_videos:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"⚠️  DESCARGA LOS VIDEOS DE UDACITY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nDescarga al menos uno de estos videos y colócalo en esta carpeta:\\n\")\n",
    "    for name, url in UDACITY_VIDEOS.items():\n",
    "        print(f\"  {name}:\")\n",
    "        print(f\"  {url}\\n\")\n",
    "else:\n",
    "    VIDEO_PATH = available_videos[0]\n",
    "    print(f\"\\n✓ Usando video: {VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clase Principal: LaneDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneDetector:\n",
    "    \"\"\"\n",
    "    Detector de líneas de carril usando técnicas clásicas de visión por computador.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Conversión a escala de grises\n",
    "    2. Suavizado Gaussiano\n",
    "    3. Detección de bordes (Canny)\n",
    "    4. Región de Interés (ROI)\n",
    "    5. Transformada de Hough\n",
    "    6. Separación izquierda/derecha por pendiente\n",
    "    7. Regresión lineal\n",
    "    8. Suavizado temporal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size=10):\n",
    "        # Buffers para suavizado temporal\n",
    "        self.left_lines_buffer = deque(maxlen=buffer_size)\n",
    "        self.right_lines_buffer = deque(maxlen=buffer_size)\n",
    "        \n",
    "        # Parámetros del detector de Canny\n",
    "        self.canny_low = 50\n",
    "        self.canny_high = 150\n",
    "        \n",
    "        # Parámetros de la transformada de Hough\n",
    "        self.hough_rho = 2\n",
    "        self.hough_theta = np.pi/180\n",
    "        self.hough_threshold = 50\n",
    "        self.hough_min_line_length = 40\n",
    "        self.hough_max_line_gap = 100\n",
    "        \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Procesa un frame completo del video.\"\"\"\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # 1. Escala de grises\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 2. Suavizado Gaussiano\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        \n",
    "        # 3. Detección de bordes Canny\n",
    "        edges = cv2.Canny(blurred, self.canny_low, self.canny_high)\n",
    "        \n",
    "        # 4. Región de interés (trapecio) - Ajustado para videos Udacity\n",
    "        mask = np.zeros_like(edges)\n",
    "        vertices = np.array([[\n",
    "            (int(width * 0.05), height),\n",
    "            (int(width * 0.45), int(height * 0.6)),\n",
    "            (int(width * 0.55), int(height * 0.6)),\n",
    "            (int(width * 0.95), height)\n",
    "        ]], dtype=np.int32)\n",
    "        cv2.fillPoly(mask, vertices, 255)\n",
    "        roi_edges = cv2.bitwise_and(edges, mask)\n",
    "        \n",
    "        # 5. Transformada de Hough\n",
    "        lines = cv2.HoughLinesP(roi_edges, self.hough_rho, self.hough_theta,\n",
    "                                self.hough_threshold, \n",
    "                                minLineLength=self.hough_min_line_length,\n",
    "                                maxLineGap=self.hough_max_line_gap)\n",
    "        \n",
    "        # 6. Separar líneas izquierda/derecha\n",
    "        left_lines, right_lines = [], []\n",
    "        if lines is not None:\n",
    "            for line in lines:\n",
    "                x1, y1, x2, y2 = line[0]\n",
    "                if x2 - x1 == 0:\n",
    "                    continue\n",
    "                slope = (y2 - y1) / (x2 - x1)\n",
    "                if abs(slope) < 0.5:\n",
    "                    continue\n",
    "                if slope < 0 and (x1 + x2) / 2 < width / 2:\n",
    "                    left_lines.append(line[0])\n",
    "                elif slope > 0 and (x1 + x2) / 2 > width / 2:\n",
    "                    right_lines.append(line[0])\n",
    "        \n",
    "        # 7. Ajustar líneas por regresión\n",
    "        left_line = self._fit_line(left_lines, height)\n",
    "        right_line = self._fit_line(right_lines, height)\n",
    "        \n",
    "        # 8. Suavizado temporal\n",
    "        left_line = self._average_line(left_line, self.left_lines_buffer)\n",
    "        right_line = self._average_line(right_line, self.right_lines_buffer)\n",
    "        \n",
    "        # Dibujar resultado\n",
    "        result = self._draw_lanes(frame.copy(), left_line, right_line)\n",
    "        \n",
    "        return result, edges, roi_edges\n",
    "    \n",
    "    def _fit_line(self, lines, img_height):\n",
    "        if not lines:\n",
    "            return None\n",
    "        points_x, points_y = [], []\n",
    "        for x1, y1, x2, y2 in lines:\n",
    "            points_x.extend([x1, x2])\n",
    "            points_y.extend([y1, y2])\n",
    "        if len(points_x) < 2:\n",
    "            return None\n",
    "        try:\n",
    "            m, b = np.polyfit(points_x, points_y, 1)\n",
    "            if m == 0:\n",
    "                return None\n",
    "            y_bottom, y_top = img_height, int(img_height * 0.6)\n",
    "            x_bottom = int((y_bottom - b) / m)\n",
    "            x_top = int((y_top - b) / m)\n",
    "            return (x_bottom, y_bottom, x_top, y_top)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _average_line(self, current_line, buffer):\n",
    "        if current_line is not None:\n",
    "            buffer.append(current_line)\n",
    "        if not buffer:\n",
    "            return None\n",
    "        return tuple(np.mean(buffer, axis=0).astype(int))\n",
    "    \n",
    "    def _draw_lanes(self, img, left_line, right_line):\n",
    "        if left_line:\n",
    "            cv2.line(img, (left_line[0], left_line[1]), (left_line[2], left_line[3]), (0, 255, 0), 10)\n",
    "        if right_line:\n",
    "            cv2.line(img, (right_line[0], right_line[1]), (right_line[2], right_line[3]), (0, 255, 0), 10)\n",
    "        if left_line and right_line:\n",
    "            pts = np.array([[left_line[0], left_line[1]], [left_line[2], left_line[3]],\n",
    "                           [right_line[2], right_line[3]], [right_line[0], right_line[1]]], np.int32)\n",
    "            overlay = img.copy()\n",
    "            cv2.fillPoly(overlay, [pts], (0, 255, 0))\n",
    "            img = cv2.addWeighted(overlay, 0.3, img, 0.7, 0)\n",
    "        return img\n",
    "    \n",
    "    def reset(self):\n",
    "        self.left_lines_buffer.clear()\n",
    "        self.right_lines_buffer.clear()\n",
    "\n",
    "print(\"✓ Clase LaneDetector cargada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cargar Video de Udacity y Mostrar Frame de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que tenemos un video\n",
    "if 'VIDEO_PATH' not in dir() or not os.path.exists(VIDEO_PATH):\n",
    "    raise FileNotFoundError(\"⚠️ Descarga primero un video de Udacity (ver celda 2)\")\n",
    "\n",
    "# Cargar información del video\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = total_frames / fps\n",
    "\n",
    "# Leer un frame de ejemplo\n",
    "ret, sample_frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"INFORMACIÓN DEL VIDEO - UDACITY DATASET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Archivo:     {VIDEO_PATH}\")\n",
    "print(f\"  Resolución:  {width}x{height}\")\n",
    "print(f\"  FPS:         {fps}\")\n",
    "print(f\"  Frames:      {total_frames}\")\n",
    "print(f\"  Duración:    {duration:.1f} segundos\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Mostrar frame de ejemplo\n",
    "if ret:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Frame de ejemplo - {VIDEO_PATH} (Udacity Dataset)')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualización del Pipeline Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline(frame, video_name, save_path='pipeline_visualization.png'):\n",
    "    \"\"\"\n",
    "    Visualiza cada paso del pipeline de detección.\n",
    "    \"\"\"\n",
    "    detector = LaneDetector()\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # Ejecutar cada paso del pipeline\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # ROI\n",
    "    mask = np.zeros_like(edges)\n",
    "    vertices = np.array([[\n",
    "        (int(width * 0.05), height),\n",
    "        (int(width * 0.45), int(height * 0.6)),\n",
    "        (int(width * 0.55), int(height * 0.6)),\n",
    "        (int(width * 0.95), height)\n",
    "    ]], dtype=np.int32)\n",
    "    cv2.fillPoly(mask, vertices, 255)\n",
    "    roi_edges = cv2.bitwise_and(edges, mask)\n",
    "    \n",
    "    # Hough lines\n",
    "    lines = cv2.HoughLinesP(roi_edges, 2, np.pi/180, 50, minLineLength=40, maxLineGap=100)\n",
    "    hough_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(hough_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Resultado final\n",
    "    result, _, _ = detector.process_frame(frame)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "    fig.suptitle(f'Pipeline de Detección de Líneas de Carril - Dataset Udacity ({video_name})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Fila 1\n",
    "    axes[0,0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,0].set_title('1. Imagen Original')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    axes[0,1].imshow(gray, cmap='gray')\n",
    "    axes[0,1].set_title('2. Escala de Grises')\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    axes[0,2].imshow(blurred, cmap='gray')\n",
    "    axes[0,2].set_title('3. Filtro Gaussiano (5x5)')\n",
    "    axes[0,2].axis('off')\n",
    "    \n",
    "    axes[0,3].imshow(edges, cmap='gray')\n",
    "    axes[0,3].set_title('4. Bordes (Canny 50-150)')\n",
    "    axes[0,3].axis('off')\n",
    "    \n",
    "    # Fila 2\n",
    "    roi_vis = frame.copy()\n",
    "    cv2.polylines(roi_vis, vertices, True, (0, 0, 255), 3)\n",
    "    axes[1,0].imshow(cv2.cvtColor(roi_vis, cv2.COLOR_BGR2RGB))\n",
    "    axes[1,0].set_title('5. Región de Interés (ROI)')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    axes[1,1].imshow(roi_edges, cmap='gray')\n",
    "    axes[1,1].set_title('6. Bordes en ROI')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    axes[1,2].imshow(cv2.cvtColor(hough_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1,2].set_title('7. Transformada de Hough')\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    axes[1,3].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    axes[1,3].set_title('8. Resultado Final')\n",
    "    axes[1,3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n✓ Visualización guardada: {save_path}\")\n",
    "    return fig\n",
    "\n",
    "# Ejecutar visualización con frame del video de Udacity\n",
    "if ret:\n",
    "    fig = visualize_pipeline(sample_frame, VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(video_path, max_frames=100):\n",
    "    \"\"\"\n",
    "    Analiza el rendimiento del detector en el video de Udacity.\n",
    "    \"\"\"\n",
    "    detector = LaneDetector()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se puede abrir el video\")\n",
    "        return None\n",
    "    \n",
    "    times = []\n",
    "    detections_left = 0\n",
    "    detections_right = 0\n",
    "    total = 0\n",
    "    \n",
    "    while total < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        start = time.time()\n",
    "        detector.process_frame(frame)\n",
    "        times.append(time.time() - start)\n",
    "        \n",
    "        if len(detector.left_lines_buffer) > 0:\n",
    "            detections_left += 1\n",
    "        if len(detector.right_lines_buffer) > 0:\n",
    "            detections_right += 1\n",
    "        total += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return {\n",
    "        'frames': total,\n",
    "        'avg_time_ms': np.mean(times) * 1000,\n",
    "        'std_time_ms': np.std(times) * 1000,\n",
    "        'fps': 1 / np.mean(times),\n",
    "        'detection_left': (detections_left / total) * 100,\n",
    "        'detection_right': (detections_right / total) * 100,\n",
    "        'times': times\n",
    "    }\n",
    "\n",
    "# Ejecutar análisis en el video de Udacity\n",
    "print(f\"Analizando rendimiento en {VIDEO_PATH}...\\n\")\n",
    "metrics = analyze_performance(VIDEO_PATH, max_frames=100)\n",
    "\n",
    "if metrics:\n",
    "    print(\"=\"*55)\n",
    "    print(\"MÉTRICAS DE RENDIMIENTO - DATASET UDACITY\")\n",
    "    print(\"=\"*55)\n",
    "    print(f\"  Video analizado:          {VIDEO_PATH}\")\n",
    "    print(f\"  Frames analizados:        {metrics['frames']}\")\n",
    "    print(f\"  Tiempo por frame:         {metrics['avg_time_ms']:.2f} ± {metrics['std_time_ms']:.2f} ms\")\n",
    "    print(f\"  FPS de procesamiento:     {metrics['fps']:.1f}\")\n",
    "    print(f\"  Detección línea izq:      {metrics['detection_left']:.1f}%\")\n",
    "    print(f\"  Detección línea der:      {metrics['detection_right']:.1f}%\")\n",
    "    print(\"=\"*55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar métricas\n",
    "if metrics:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(f'Métricas de Rendimiento - {VIDEO_PATH} (Udacity)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Gráfico de tiempos\n",
    "    axes[0].plot(metrics['times'], 'b-', alpha=0.7, linewidth=0.8)\n",
    "    axes[0].axhline(y=np.mean(metrics['times']), color='r', linestyle='--', \n",
    "                    label=f'Media: {metrics[\"avg_time_ms\"]:.2f} ms')\n",
    "    axes[0].set_xlabel('Frame')\n",
    "    axes[0].set_ylabel('Tiempo (segundos)')\n",
    "    axes[0].set_title('Tiempo de Procesamiento por Frame')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico de tasas de detección\n",
    "    categories = ['Línea Izquierda', 'Línea Derecha']\n",
    "    rates = [metrics['detection_left'], metrics['detection_right']]\n",
    "    colors = ['#27ae60', '#3498db']\n",
    "    \n",
    "    bars = axes[1].bar(categories, rates, color=colors)\n",
    "    axes[1].set_ylabel('Tasa de Detección (%)')\n",
    "    axes[1].set_title('Tasa de Detección de Líneas')\n",
    "    axes[1].set_ylim(0, 105)\n",
    "    \n",
    "    for bar, rate in zip(bars, rates):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                    f'{rate:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metricas_rendimiento.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✓ Gráficos guardados: metricas_rendimiento.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Procesar Video Completo de Udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Procesa el video completo de Udacity y guarda el resultado.\n",
    "    \"\"\"\n",
    "    detector = LaneDetector()\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: No se puede abrir {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Procesando: {input_path}\")\n",
    "    print(f\"Video: {width}x{height} @ {fps}fps, {total} frames\")\n",
    "    \n",
    "    # Codec XVID (más compatible)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    if not writer.isOpened():\n",
    "        print(\"Error: No se puede crear el video de salida\")\n",
    "        return False\n",
    "    \n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        result, _, _ = detector.process_frame(frame)\n",
    "        writer.write(result)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            progress = 100 * count / total\n",
    "            print(f\"  Progreso: {count}/{total} frames ({progress:.1f}%)\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    \n",
    "    print(f\"\\n✓ Completado: {count} frames en {elapsed:.1f}s\")\n",
    "    print(f\"✓ Video guardado: {output_path}\")\n",
    "    return True\n",
    "\n",
    "# Procesar el video de Udacity\n",
    "output_video = 'output_' + VIDEO_PATH.replace('.mp4', '.avi')\n",
    "process_video(VIDEO_PATH, output_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mostrar Frames de Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar algunos frames del video procesado\n",
    "if os.path.exists(output_video):\n",
    "    cap = cv2.VideoCapture(output_video)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Seleccionar 4 frames distribuidos\n",
    "    frame_indices = [int(total * i / 5) for i in range(1, 5)]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Resultados de Detección - {VIDEO_PATH} (Udacity)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, (ax, frame_idx) in enumerate(zip(axes.flat, frame_indices)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f'Frame {frame_idx}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    cap.release()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resultados_deteccion.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✓ Resultados guardados: resultados_deteccion.png\")\n",
    "else:\n",
    "    print(f\"No se encontró el video procesado: {output_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen y Conclusiones\n",
    "\n",
    "### Dataset Utilizado\n",
    "\n",
    "**Udacity Self-Driving Car Nanodegree - Lane Lines Detection**\n",
    "- Repositorio: https://github.com/udacity/CarND-LaneLines-P1\n",
    "- Videos: solidWhiteRight.mp4, solidYellowLeft.mp4\n",
    "- Resolución: 960x540 @ 25 FPS\n",
    "- Condiciones: Autopista, luz diurna, líneas blancas/amarillas\n",
    "\n",
    "### Técnicas Implementadas\n",
    "\n",
    "1. **Conversión a escala de grises**: Reduce complejidad de 3 canales a 1\n",
    "2. **Filtro Gaussiano (5x5)**: Reduce ruido preservando bordes\n",
    "3. **Detector de bordes Canny (50-150)**: Umbralización con histéresis\n",
    "4. **Región de Interés (ROI)**: Trapecio enfocado en el carril\n",
    "5. **Transformada de Hough**: Detección de segmentos de línea\n",
    "6. **Clasificación por pendiente**: Separación izquierda/derecha\n",
    "7. **Regresión lineal**: Ajuste de múltiples segmentos\n",
    "8. **Suavizado temporal**: Buffer de 10 frames\n",
    "\n",
    "### Limitaciones Identificadas\n",
    "\n",
    "- Sensibilidad a cambios bruscos de iluminación\n",
    "- Dificultades con curvas pronunciadas\n",
    "- Modelo asume líneas rectas\n",
    "- Puede fallar con oclusiones\n",
    "\n",
    "### Posibles Mejoras\n",
    "\n",
    "- Transformación de perspectiva (bird's eye view)\n",
    "- Ajuste polinomial de mayor grado para curvas\n",
    "- Filtro de Kalman para predicción temporal\n",
    "- Segmentación por color (HSV) para líneas amarillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROYECTO COMPLETADO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: Udacity Self-Driving Car Nanodegree\")\n",
    "print(f\"Video procesado: {VIDEO_PATH}\")\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"  - {output_video}: Video con detección de carriles\")\n",
    "print(\"  - pipeline_visualization.png: Visualización del pipeline\")\n",
    "print(\"  - metricas_rendimiento.png: Gráficos de métricas\")\n",
    "print(\"  - resultados_deteccion.png: Frames de ejemplo\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
